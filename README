ğŸ“Œ Overview

This project provides an infrastructure-as-code solution for deploying a highly available 
Amazon EKS (Elastic Kubernetes Service) cluster across two Availability Zones using AWS 
CloudFormation. The setup includes an EC2 Bastion Host to provide secure SSM access for 
managing the cluster and related resources.

ğŸ› ï¸ Features

    ğŸ—ï¸ Deploys a fully managed EKS cluster on AWS

    ğŸŒ Spans across two Availability Zones for high availability

    ğŸ” EC2 Bastion Host for secure management access

    ğŸ”„ Supports cluster upgrades and scalability

    ğŸ§© Modular CloudFormation templates for reusability

    ğŸƒ Automated deployment of Kubernets cluster, monitoring namespace, alerting, ingress 
    and application for complete end to end automation. 

ğŸ§± Architecture

    EKS Cluster

        Control plane managed by AWS

        Worker nodes in private subnets across 2 AZs

        2 Node groups, one for monitoring and the other for the application and core Kube 
            functions 

    VPC

        Custom VPC with public, private and protected subnets

        NAT Gateway setup

    Bastion Host

        EC2 instance in a private subnet

        Used for secure management access via SSM 

ğŸš€ Deployment

    You must have some things set up before you can run the start up script 

        1. Create a key, secret key pair on a user that has the appropriate permissions to run kubectl, access SSM, and assume identities. I used the AWS provided "AdministratorAccess" permissions, the internet strongly advises me not to use the root account. 

        Either create a user, or use one you already have with robust permissions, and in the IAM console create a key pair for them under "Security Credentials". 
        
        (This is also advised against and you will get a warning, AWS advises that you grant permissions by assigning roles, not by using access keys, if I had created the bastion first and then deployed the cluster from there through the pipeline, rather than by deploying from a personal computer, you would probably not need to use an access key. But you can deactivate the key after you are done.)

        Configure the profile of this user on your personal computer. You'll need to install the AWS CLI, this is done differently depending on your OS, so I'll leave that to you. 

        these are the commands to set up an AWS profile 

            aws configure --profile doggy-demo-profile 

        This will prompt you for the <access key ID> <Secret Access key> <default region name> <default output format>

        the access key and secret access key will be the ones you just made in the console. The default region will be "us-east-1" and output can be "yaml" or "json" whichever you prefer. I like Yaml. 

        2. Create an API key on OpenAI and save it to a secret with the key value pair being "key, <apikey>" it is important that you make the value "key" becuase that is how i coded it in the alert manager script (though writing this now you can save secrets as single strings and that would have been better, you can see in "KubernetesArchitecture/alerting/prometheus-webhook.py). 

        I'll leave it up to you to collect the apikey from open AI, if create a user at https://openai.com/api/ they have made it very easy to create an API key. 

        You will need to load money onto OpenAI to use the platform, I configured my app to use the cheapest option, 2 dollars should be more than enough for a demonstration. 

        Once you have the key and the funds loaded, take that key to AWS secrets manager in the AWS console. You don't want this key to get out into the open so it is loaded into the application from the ARN of the key. 
    
    Once this is done... 

        git clone https://github.com/DoggyApp/CloudFormationArchitecture.git
        cd CloudFormationArchitecture

        aws cloudformation package \
            --template-file master.yaml \
            --s3-bucket doggy-template-build-terminal \
            --output-template-file packaged-root-template.yaml \ 
            --profile doggy-demo-profile

        aws cloudformation validate-template \
            --template-body file://packaged-root-template.yaml \
            --profile doggy-demo-profile

        aws cloudformation deploy \
          --template-file packaged-root-template.yaml \
          --stack-name doggy-stack \
          --capabilities CAPABILITY_NAMED_IAM \
          --region us-east-1 \ 
          --parameter-overrides \ 
            AdminAccessKey=<access_key> \ # same one used in this doggy-demo-profile 
            AdminSecretKey=<secret_access_key> \ # """"
            email=<email> \ # use the email you would like to recieve alerts to 
            AwsId=<AwsId> \ # your AWS ID 
            OpenAiARN=<SecretARN> # the ARN of the secret you created above 

    The script should take it from here to deploy everything, there is an ansible script that setsup the environment on the start up of the EC2 management instance between that and EKS spinning up, it will take around 30 min to get everything up and running. But you should recieve an email confirming the subscription to "doggy-alerts". 

    In the EC2 startup script I define some useful functions in the .bashrc file. One is 

        startup 

    This will automatically run the Ansible script to setup the environment. 

    the other is 

        teardown 

    This will delete the entire environment including subscriptions (if you tear it down without confirming the subscription you will be unable to delete it, though the AWS garbage collector will delete it within 2 to 5 days of the SNS topic being deleted, but will be mildly annoying for that time. So be warned!) 

    for a walk though of parusing that environment look to the KubernetesArchitecture README! 

    Remeber to deavtivate the access key when you are done! AWS will not automatically deactivate them if they go unused, but they will ping a security recommnedation on your IAM page when you next look at it. 

ğŸ›¡ï¸ Security 

    ğŸ” Private subnets are used to restrict access to the Kubernetes cluster and worker nodes 
    are placed in the private subet. 

    ğŸ” Security group on mgmt instance blocks all traffic, only allowing SSM, and is placed in
    a private subnet to ensure no public access. 

    ğŸ” Security group on Kubernetes cluster blocks all traffic, except TCP originating from mgmt 
    instance security group. 

    ğŸ” Mgmt instance role only given access to doggy-app-cluster resource. 

    ğŸ” Access Key and Secret Access Key (needed to access kubernetes cluster) stored as secrets 
    and pulled from AWS secrets manager during build. 

    ğŸ” KMS used to encrypt block storage on mgmt instance. 

    ğŸ” AWS profile uses admin user, not root, all users are locked behind MFA. 

    ğŸ” Only admin user has permission to assume role of the pipeline and access Kube Cluster. 




    
